{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import uuid\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "\n",
    "class cfg:\n",
    "    exp_id = \"1001\" # 实验ID\n",
    "    seed = 42 # 随机种子\n",
    "    data_path = \"/home/xm/workspace/nbme-score-clinical-patient-notes/patient_notes.csv\" # notes数据\n",
    "    pretrained_checkpoint = 'microsoft/deberta-base' # microsoft/deberta-large /  microsoft/deberta-v3-large\n",
    "    lr = 1e-5\n",
    "    batch_size = 32\n",
    "    epochs = 10 \n",
    "    save_total_limit = 2 # 最多checkpoint的数量\n",
    "    mlm_prob = 0.2 # mlm概率\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    '''\n",
    "    设置随机种子，方便实验复现\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.data_path) # 读取notes数据\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_checkpoint, trim_offsets=False) # 分词 tokenizer # trim_offsets==False 删除因offsets造成的空白token\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, lines, block_size):\n",
    "        batch_encoding = tokenizer(\n",
    "                                    lines, # 文本\n",
    "                                    add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                                    truncation=True, # 文本截断，则将其截断为max_length参数指定的最大长度.\n",
    "                                    max_length=block_size, # 文本最大长度\n",
    "                                  )\n",
    "        self.examples = batch_encoding[\"input_ids\"] # 获取 input_ids\n",
    "        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples) # 样本长度\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i] # 返回指定 input_ids\n",
    "\n",
    "dataset = LineByLineTextDataset(tokenizer, df['pn_history'].tolist(), 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=f\"/home/xm/workspace/output/{cfg.exp_id}\", # 保存路径\n",
    "    save_strategy=\"epoch\", # 以epoch频率保存模型\n",
    "    learning_rate=cfg.lr, # 学习率\n",
    "    per_device_train_batch_size=cfg.batch_size, \n",
    "    per_device_eval_batch_size=cfg.batch_size, \n",
    "    num_train_epochs=cfg.epochs, \n",
    "    warmup_ratio=0.2, # 初始学习率倍数\n",
    "    fp16=True, # 混合精度\n",
    "    dataloader_num_workers=4, # cpu线程数\n",
    "    group_by_length=True, # 使用动态padding 更快的训练\n",
    "    run_name=cfg.exp_id, # 实验ID\n",
    "    save_total_limit=cfg.save_total_limit if cfg.save_total_limit>0 else None, # 最多checkpoint的数量\n",
    "    seed=cfg.seed, # 随机种子\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(cfg.pretrained_checkpoint) # MLM 模型\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, # 模型\n",
    "    args, # 超参数\n",
    "    train_dataset=dataset, # 数据集\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=cfg.mlm_prob), # 数据整理器\n",
    ")\n",
    "\n",
    "trainer.train() # 开始pretrain"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e16852016d24c6c850171da06836ee6dec9d765a18cae5c88840c842c8f803b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
