{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T13:18:21.683040Z",
     "iopub.status.busy": "2022-05-03T13:18:21.682773Z",
     "iopub.status.idle": "2022-05-03T13:18:27.851111Z",
     "shell.execute_reply": "2022-05-03T13:18:27.850383Z"
    },
    "papermill": {
     "duration": 6.180508,
     "end_time": "2022-05-03T13:18:27.853257",
     "exception": false,
     "start_time": "2022-05-03T13:18:21.672749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "BS = 4\n",
    "\n",
    "# 模型结构\n",
    "CHECKPOINTS = [\n",
    "    '../input/deberta-v3-large', #https://www.kaggle.com/datasets/jonathanchan/deberta-v3-large\n",
    "    '../input/microsoft-deberta-large', # https://www.kaggle.com/datasets/shinomoriaoshi/microsoft-deberta-large\n",
    "    '../input/debertabase',  # https://www.kaggle.com/datasets/chenjinbridge/debertabase\n",
    "]\n",
    "\n",
    "# 权重文件\n",
    "PATHS = [\n",
    "    '../input/my-deberta-v3-large',\n",
    "    '../input/my-deberta-large',\n",
    "    '../input/my-deberta-base',\n",
    "]\n",
    "\n",
    "# 加权\n",
    "WEIGHTS = {'w0': 0.6, \n",
    "           'w1': 0.25, \n",
    "           'w2': 0.15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T13:18:27.871727Z",
     "iopub.status.busy": "2022-05-03T13:18:27.871533Z",
     "iopub.status.idle": "2022-05-03T13:18:28.772599Z",
     "shell.execute_reply": "2022-05-03T13:18:28.771798Z"
    },
    "papermill": {
     "duration": 0.912577,
     "end_time": "2022-05-03T13:18:28.774560",
     "exception": false,
     "start_time": "2022-05-03T13:18:27.861983",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv') # 测试数据\n",
    "features = preprocess_features(pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')) # features\n",
    "pn = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv') # notes\n",
    "\n",
    "# 合并到一张表\n",
    "test_df = test_df.merge(pn, on='pn_num', how='left') \n",
    "test_df = test_df.merge(features, on='feature_num', how='left')\n",
    "test_df['len'] = test_df['pn_history'].apply(len) + test_df['feature_text'].apply(len)\n",
    "test_df = test_df.sort_values(by=['len']).reset_index(drop=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\" # 修正feature_text\n",
    "    return features\n",
    "\n",
    "\n",
    "def prepare_input(tokenizer, text, feature_text):\n",
    "    '''\n",
    "    构造 input 数据\n",
    "    '''\n",
    "    inputs = tokenizer(text, feature_text, #note and feature\n",
    "                       add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                       return_offsets_mapping=False # 将每个tokens映射回原始文本character级别的位置。\n",
    "                      )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "class NBMEDatasetInfer(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_texts = df['feature_text'].values # feature_text\n",
    "        self.pn_historys = df['pn_history'].values  # notes_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts) # 样本数\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.tokenizer,\n",
    "                               self.pn_historys[item],\n",
    "                               self.feature_texts[item]\n",
    "                              )\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super().__init__()\n",
    "        # output_hidden_states=True 返回所有层的隐藏状态。 返回值为 hidden_states。\n",
    "        # hidden_states 是一个元组，它的第一个元素是Embedding，其余元素是各层的输出，shape == [bs, seq_len, hidden_size]\n",
    "        self.config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True) # AutoConfig\n",
    "        self.backbone = AutoModel.from_pretrained(checkpoint) # AutoModel\n",
    "        self.dropout = nn.Dropout(0.1) # Dropout\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1) #MLP\n",
    "        self._init_weights(self.classifier) # 初始化 \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Linear 层\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Embedding 层 \n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)  # initializer_range: 0.02\n",
    "            if module.padding_idx is not None:\n",
    "                # padding部分置零\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm): \n",
    "            # Normalization层 \n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        #inputs(dict)\n",
    "        #    input_ids, token_type_ids, attention_mask, label\n",
    "        outputs = self.backbone(**{k: v for k, v in inputs.items() if k != 'label'})\n",
    "        # outputs: [last_hidden_state], last_hidden_state: [bs, seq_len, hidden_size]\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(self.dropout(sequence_output)) #获得 preds\n",
    "        loss = None\n",
    "        if 'label' in inputs:\n",
    "            # 计算loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "            loss = loss_fct(logits.view(-1, 1), inputs['label'].view(-1, 1).float())\n",
    "            loss = torch.masked_select(loss, inputs['label'].view(-1, 1) != -100).mean()\n",
    "        # 返回值\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, # loss\n",
    "            logits=logits, # logits\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_logits(texts, predictions, tokenizer):\n",
    "    '''\n",
    "    获得每个char级的预测概率值\n",
    "    texts: 原始notes文本数据(会重复，notes对应多个features)\n",
    "    predictions: token级预测概率值\n",
    "    '''\n",
    "    results = [np.zeros(len(t)) for t in texts] # 输出列表 [[0,0,0],[0,0,0,0]]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, # note\n",
    "                            add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                            return_offsets_mapping=True # 将每个tokens映射回原始文本char级别的位置。\n",
    "                           )\n",
    "        offset_mappings = encoded['offset_mapping']\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset_mappings, prediction)):\n",
    "            start, end = offset_mapping\n",
    "            results[i][start:end] = pred # char级填上logits\n",
    "    return results\n",
    "\n",
    "def my_get_results(char_logits, texts, th=0):\n",
    "    '''\n",
    "    生成所有样本的span字符串 of list，同一样本的span用;隔开\n",
    "    '''\n",
    "    results = []\n",
    "    for i, char_prob in enumerate(char_logits): # 循环所有样本\n",
    "        result = np.where(char_prob > th)[0] # 大于阈值的索引值\n",
    "        # 根据数值是否连续进行分组\n",
    "        # result: array([  0,   1,  90,  91,  92,  93,  94,  95,  96,  97,  98, 628, 629, 630])\n",
    "        # to\n",
    "        # result: [[0, 1], [90, 91, 92, 93, 94, 95, 96, 97, 98], [628, 629, 630]]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        temp = []\n",
    "        for r in result:\n",
    "            s, e = min(r), max(r)\n",
    "            while texts[i][s] == ' ': # 去掉左侧空格\n",
    "                s += 1 \n",
    "            while texts[i][e] == ' ': # 去掉右侧空格\n",
    "                e -= 1\n",
    "            temp.append(f\"{s} {e+1}\")\n",
    "        result = temp\n",
    "        result = \";\".join(result) # 加入;后保存\n",
    "        results.append(result)\n",
    "        \n",
    "    #  results like ['0 5;64 72', '91 99', '128 134']\n",
    "    return results\n",
    "\n",
    "def get_predictions(results):\n",
    "    '''\n",
    "    span 字符串 转 list\n",
    "    from ['0 5;64 72', '91 99', '128 134']  \n",
    "    return [[[0, 5], [64, 72]], [[91, 99]], [[128, 134]]]\n",
    "    '''\n",
    "    predictions = []\n",
    "    for result in results: # 循环所有样本的span字符串\n",
    "        prediction = []\n",
    "        if result != \"\": # 非空span\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T13:18:28.794795Z",
     "iopub.status.busy": "2022-05-03T13:18:28.794100Z",
     "iopub.status.idle": "2022-05-03T13:24:28.899942Z",
     "shell.execute_reply": "2022-05-03T13:24:28.898524Z"
    },
    "papermill": {
     "duration": 360.118338,
     "end_time": "2022-05-03T13:24:28.902424",
     "exception": false,
     "start_time": "2022-05-03T13:18:28.784086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_logits_blend = [np.zeros(len(text)) for text in test_df.pn_history.values] # blend后，char级的预测概率值\n",
    "for i, ckpt in enumerate(CHECKPOINTS): # 循环所有融合的模型\n",
    "    model_path = PATHS[i] # 模型路径\n",
    "    w = WEIGHTS[f'w{i}'] # 加权\n",
    "    print(f'{model_path} - weight = {w}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt, trim_offsets=False) # Tokenizer\n",
    "    test_dataset = NBMEDatasetInfer(tokenizer, test_df) # datasets\n",
    "    maxlen = max([len(x['input_ids']) for x in test_dataset]) # 最长样本的len作为maxlen\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BS, shuffle=False, collate_fn=DataCollatorForTokenClassification(tokenizer), pin_memory=True)\n",
    "    model = NBMEModel(ckpt).cuda()  # 创建模型\n",
    "    preds_folds = []\n",
    "\n",
    "    for fold in range(5):\n",
    "        model.load_state_dict(torch.load(os.path.join(model_path, f'{fold}.pt'))) # 载入模型权重\n",
    "        model.eval() # 评估模式\n",
    "        preds = []\n",
    "        for b in tqdm(test_dataloader, total=len(test_dataset)//BS+1):\n",
    "            b = {k: v.cuda() for k, v in b.items()} # batch\n",
    "            pred = model(**b).logits #[bs, maxlen, 1]\n",
    "            pred = pred.view(pred.shape[0], pred.shape[1]) #[bs, maxlen]\n",
    "            pred = F.pad(input=pred, pad=(0, maxlen-pred.shape[1]), mode='constant', value=-100).cpu().numpy() # pad满maxlen，填充值-100 \n",
    "            preds.append(pred)\n",
    "        preds = np.concatenate(preds, axis=0)   # 所有样本的预测值 # [n, maxlen]\n",
    "        preds_folds.append(preds) # 单fold 所有预测值\n",
    "    preds_folds = np.stack(preds_folds)\n",
    "    print('preds_folds shape:', preds_folds.shape)\n",
    "    preds = np.mean(preds_folds, axis=0) \n",
    "    char_logits = get_char_logits(test_df['pn_history'].values, preds, tokenizer) # 获得char级的预测概率值\n",
    "    for j in range(len(test_df)):\n",
    "        char_logits_blend[j] += w * char_logits[j] # 加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T13:24:28.972949Z",
     "iopub.status.busy": "2022-05-03T13:24:28.972697Z",
     "iopub.status.idle": "2022-05-03T13:24:29.007695Z",
     "shell.execute_reply": "2022-05-03T13:24:29.006904Z"
    },
    "papermill": {
     "duration": 0.072921,
     "end_time": "2022-05-03T13:24:29.009936",
     "exception": false,
     "start_time": "2022-05-03T13:24:28.937015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = my_get_results(char_logits_blend, test_df.pn_history.values, th=0) # 生成所有样本的span字符串 of list\n",
    "\n",
    "# 生成 submission.csv ，用作提交\n",
    "test_df['location'] = results \n",
    "sub = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/sample_submission.csv\")\n",
    "sub = sub[['id']].merge(test_df[['id', \"location\"]], how=\"left\", on=\"id\")\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.032418,
     "end_time": "2022-05-03T13:24:29.076999",
     "exception": false,
     "start_time": "2022-05-03T13:24:29.044581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 417.384237,
   "end_time": "2022-05-03T13:24:32.077795",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-03T13:17:34.693558",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
